{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import wykop\n",
    "import collections\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from cfg import appkey, secretkey, acckey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap comments from single entry (no API keys required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_color(color):\n",
    "    if color.startswith('color'):\n",
    "        return int(color.split('-')[-1])\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def get_comments_from_entry(entry_id, topic=None):\n",
    "    try:\n",
    "        URL = f'https://www.wykop.pl/wpis/{entry_id}'\n",
    "        page = requests.get(URL)\n",
    "\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        body = soup.find('body')\n",
    "        ultag = body.find_all('ul', {'class': 'comments-stream'})\n",
    "\n",
    "        comments_list = []\n",
    "        if ultag:\n",
    "            ultag = ultag[0]\n",
    "            litag = ultag.find_all('li')[0]\n",
    "            ultag2 = body.find('ul', {'class': 'sub'})\n",
    "            comment = ultag2.find_all('div', {'class': ['wblock lcontrast dC authorComment', 'wblock lcontrast dC']})\n",
    "            for ul in comment:\n",
    "                author = ul.find('div', {'class': 'author ellipsis'})\n",
    "                author_login = author.find('b').get_text()\n",
    "                if (color := author.a.attrs.get(\"class\")) is not None:\n",
    "                    color = color[0]\n",
    "                else:\n",
    "                    color = author.b.attrs.get(\"class\")[0]\n",
    "\n",
    "                author_color = parse_color(color)\n",
    "                author_sex = ul.img.attrs.get('class')[1]\n",
    "                blocked = False\n",
    "                vote_count = author.find('p', {'class': 'vC'}).attrs.get('data-vc')\n",
    "                comments_count = None\n",
    "                content = ul.find('div', {'class': 'text'}).find('p').get_text().strip()\n",
    "                date = datetime.strptime(author.find('time').attrs.get('title'), \"%Y-%m-%d %H:%M:%S\")\n",
    "                with_image = (media_content := ul.find('div', {'class': 'media-content'})) is not None\n",
    "                image_url = media_content.a.attrs.get('href') if with_image else None\n",
    "\n",
    "                entry_data = {\n",
    "                    'id': entry_id,\n",
    "                    'type': 'comment',\n",
    "                    'content': content,\n",
    "                    'date': date,\n",
    "                    'author': author_login,\n",
    "                    'author_color': author_color,\n",
    "                    'author_sex': author_sex,\n",
    "                    'blocked': blocked,\n",
    "                    'vote_count': vote_count,\n",
    "                    'comments_count': comments_count,\n",
    "                    'with_image': with_image,\n",
    "                    'image_url': image_url\n",
    "                }\n",
    "\n",
    "                if topic is not None:\n",
    "                    entry_data['topic'] = topic\n",
    "\n",
    "                comments_list.append(entry_data)\n",
    "        return comments_list\n",
    "    except Exception as e:\n",
    "       print(entry_id, e)\n",
    "       return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrap entries content (without comments) from search engine using using Wykop API v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wykop.WykopAPI(appkey, secretkey)\n",
    "api.authenticate(acckey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data_from_entry(entry, topic=None):\n",
    "    entry_id = entry['id']\n",
    "    content = entry['body']\n",
    "    date = entry['date']\n",
    "    author = entry['author']\n",
    "    vote_count = entry['vote_count']\n",
    "    comments_count = entry['comments_count'] if 'comments_count' in entry.keys() else None\n",
    "    with_image = entry['embed']['type'] == 'image' if 'embed' in entry.keys() else False\n",
    "    image_url = entry['embed']['url'] if with_image else None\n",
    "\n",
    "    entry_data = {\n",
    "        'id': entry_id,\n",
    "        'type': 'entry',\n",
    "        'content': content,\n",
    "        'date': date,\n",
    "        'author': author['login'],\n",
    "        'author_color': author['color'],\n",
    "        'author_sex': author['sex'] if 'sex' in author.keys() else None, \n",
    "        'blocked': entry['blocked'],\n",
    "        'vote_count': vote_count,\n",
    "        'comments_count': comments_count,\n",
    "        'with_image': with_image,\n",
    "        'image_url': image_url,\n",
    "    }\n",
    "\n",
    "    if topic is not None:\n",
    "        entry_data['topic'] = topic\n",
    "\n",
    "    return entry_data\n",
    "\n",
    "def get_dataset(keyword_group, min_page, max_page, max_date, users_activity_counts, with_comments):\n",
    "    transformed_response = []\n",
    "    end = False\n",
    "    keyword, search_query = keyword_group\n",
    "    for page in range(min_page, max_page+1):\n",
    "        response = api.search_entries(page=page, query=search_query)\n",
    "        for post in response:\n",
    "            date_str = datetime.strptime(post['date'], \"%Y-%m-%d %H:%M:%S\")\n",
    "            if date_str < max_date:\n",
    "                print(f'Stopping {keyword} on page {page}, last text date: {date_str}, max date: {max_date}')\n",
    "                end = True\n",
    "                break\n",
    "\n",
    "            entry_data = parse_data_from_entry(post, topic=keyword)\n",
    "            transformed_response.append(entry_data)\n",
    "            users_activity_counts[entry_data['author']][keyword] += 1\n",
    "\n",
    "            if entry_data['comments_count'] > 0 and with_comments:\n",
    "                comments = get_comments_from_entry(entry_data['id'], topic=keyword)\n",
    "                for comm in comments:\n",
    "                    transformed_response.append(comm)\n",
    "                    users_activity_counts[comm['author']][keyword] += 1\n",
    "\n",
    "        if end:\n",
    "            break\n",
    "\n",
    "    dataset = pd.DataFrame(transformed_response)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Users selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_active_users(users_activity_counts, keywords, min_activity_per_keyword, min_activity_sum, verbose=False):\n",
    "    most_active_users = []\n",
    "    for user, activity in users_activity_counts.items():\n",
    "        end = False\n",
    "        activity_sum = 0\n",
    "        for key in keywords:\n",
    "            if activity[key] < min_activity_per_keyword:\n",
    "              end = True\n",
    "              break\n",
    "            else:\n",
    "                activity_sum += activity[key]\n",
    "\n",
    "        if not end and activity_sum > min_activity_sum:\n",
    "            most_active_users.append(user)\n",
    "      \n",
    "    if verbose:\n",
    "        print('Before: ', len(users_activity_counts))\n",
    "        print('After: ', len(most_active_users))\n",
    "    return most_active_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from pathlib import Path \n",
    "\n",
    "def save_results(users_activity_counts, dset):\n",
    "    Path(\"results\").mkdir(parents=True, exist_ok=True)\n",
    "    current_date = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    with open(f'results/users_activity_{current_date}.json', 'w') as f:\n",
    "        json_object = json.dumps(users_activity_counts, indent=4)\n",
    "        f.write(json_object)\n",
    "\n",
    "    dset.to_csv(f'results/{current_date}.csv', index=False, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1082/190458898.py:56: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype=datetime64[ns])\n",
      "  dataset = pd.DataFrame(transformed_response)\n"
     ]
    }
   ],
   "source": [
    "def do_scrapping(keyword_groups, keywords, min_page, max_page):\n",
    "    users_activity_counts = collections.defaultdict(lambda: collections.defaultdict(lambda: 0))\n",
    "    datasets = []\n",
    "\n",
    "    for keyword_group in keyword_groups:\n",
    "        dataset = get_dataset(keyword_group, \n",
    "            min_page=MIN_PAGE, \n",
    "            max_page=MAX_PAGE, \n",
    "            max_date = datetime(2022, 1, 30), \n",
    "            users_activity_counts=users_activity_counts, with_comments=True)\n",
    "        datasets.append(dataset)\n",
    "\n",
    "    return datasets, users_activity_counts\n",
    "\n",
    "keyword_groups = [('lewandowski', '#mecz lewandowski lewandowskiego lewy lewego robercik robercika robert')]\n",
    "keywords = ['lewandowski']\n",
    "MIN_PAGE = 1\n",
    "MAX_PAGE = 5\n",
    "MIN_ACTIVITY_PER_KEYWORD = 0\n",
    "MIN_ACTIVITY_SUM = 5\n",
    "\n",
    "results, users_activity_counts = do_scrapping(keyword_groups, keywords, MIN_PAGE, MAX_PAGE)\n",
    "accepted_users = get_most_active_users(users_activity_counts, keywords, MIN_ACTIVITY_PER_KEYWORD, MIN_ACTIVITY_SUM)\n",
    "\n",
    "final_results = []\n",
    "for result in results:\n",
    "    result = result.loc[result.author.isin(accepted_users)]\n",
    "    final_results.append(result)\n",
    "\n",
    "result_dset = pd.concat(final_results)\n",
    "result_dset = result_dset.drop_duplicates()\n",
    "save_results(users_activity_counts, result_dset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75af724b95bf5f3c3e9acc3f574214fef467343707693447c688d7bb2e03240d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
